{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import new DataFrame and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>63.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.0.1</th>\n",
       "      <th>145.0</th>\n",
       "      <th>233.0</th>\n",
       "      <th>1.0.2</th>\n",
       "      <th>2.0</th>\n",
       "      <th>150.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.3</th>\n",
       "      <th>3.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>6.0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   63.0  1.0  1.0.1  145.0  233.0  1.0.2  2.0  150.0  0.0  2.3  3.0 0.0.1  \\\n",
       "0  67.0  1.0    4.0  160.0  286.0    0.0  2.0  108.0  1.0  1.5  2.0   3.0   \n",
       "1  67.0  1.0    4.0  120.0  229.0    0.0  2.0  129.0  1.0  2.6  2.0   2.0   \n",
       "2  37.0  1.0    3.0  130.0  250.0    0.0  0.0  187.0  0.0  3.5  3.0   0.0   \n",
       "3  41.0  0.0    2.0  130.0  204.0    0.0  2.0  172.0  0.0  1.4  1.0   0.0   \n",
       "4  56.0  1.0    2.0  120.0  236.0    0.0  0.0  178.0  0.0  0.8  1.0   0.0   \n",
       "\n",
       "   6.0  0  \n",
       "0  3.0  2  \n",
       "1  7.0  1  \n",
       "2  3.0  0  \n",
       "3  3.0  0  \n",
       "4  3.0  0  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_cleveland_df = pd.read_csv(\"processed.cleveland.data\")\n",
    "og_cleveland_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. (2 pts) What is inductive reasoning? Deductive reasoning? Give an example of each, different from the examples given in class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inductive Reasoning** is a method of reasoning in which conclusions are drawn based on patterns, regularities, or specific instances. It involves making broad generalizations from specific observations.\n",
    "\n",
    "*Example:*  \n",
    "After observing that the first ten apples you picked from an orchard were ripe and sweet, you conclude that all the apples in that orchard are likely ripe and sweet.\n",
    "\n",
    "---\n",
    "\n",
    "**Deductive Reasoning** is a logical process where a conclusion is based on the concordance of multiple premises that are generally assumed to be true. It moves from a general statement to a specific conclusion with certainty.\n",
    "\n",
    "*Example:*  \n",
    "- **Premise 1:** All reptiles are cold-blooded.\n",
    "- **Premise 2:** A turtle is a reptile.\n",
    "- **Conclusion:** Therefore, a turtle is cold-blooded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. (8 pts) Preprocess your dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame with column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved with column names: processed.heading.cleveland.data\n"
     ]
    }
   ],
   "source": [
    "def new_file_with_headings(original_file_path: str, new_file_path: str, column_names: List[str]) -> None:\n",
    "    '''\n",
    "    Load a dataset without headers, assign new column names, and save it to a new file.\n",
    "\n",
    "    Parameters:\n",
    "    original_file_path (str): The path to the original file without headers.\n",
    "    new_file_path (str): The path to save the new file with headers.\n",
    "    column_names (List[str]): A list of column names to assign to the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    # Load the dataset without headers\n",
    "    df = pd.read_csv(original_file_path, header=None)\n",
    "    \n",
    "    # Assign the column names to the DataFrame\n",
    "    df.columns = column_names\n",
    "    \n",
    "    # Save the updated DataFrame to the new file with headers\n",
    "    df.to_csv(new_file_path, index=False, header=True)\n",
    "    print(f\"File saved with column names: {new_file_path}\")\n",
    "\n",
    "# Name of the columns\n",
    "column_names: List[str] = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"\n",
    "]\n",
    "\n",
    "# Call the function for the different data from each location\n",
    "new_file_with_headings(\"processed.cleveland.data\", \"processed.heading.cleveland.data\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0   63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1   67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2   67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3   37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4   41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "..   ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
       "95  52.0  1.0  4.0     128.0  255.0  0.0      0.0    161.0    1.0      0.0   \n",
       "96  59.0  1.0  4.0     110.0  239.0  0.0      2.0    142.0    1.0      1.2   \n",
       "97  60.0  0.0  4.0     150.0  258.0  0.0      2.0    157.0    0.0      2.6   \n",
       "98  52.0  1.0  2.0     134.0  201.0  0.0      0.0    158.0    0.0      0.8   \n",
       "99  48.0  1.0  4.0     122.0  222.0  0.0      2.0    186.0    0.0      0.0   \n",
       "\n",
       "    slope   ca thal  num  \n",
       "0     3.0  0.0  6.0    0  \n",
       "1     2.0  3.0  3.0    2  \n",
       "2     2.0  2.0  7.0    1  \n",
       "3     3.0  0.0  3.0    0  \n",
       "4     1.0  0.0  3.0    0  \n",
       "..    ...  ...  ...  ...  \n",
       "95    1.0  1.0  7.0    1  \n",
       "96    2.0  1.0  7.0    2  \n",
       "97    2.0  2.0  7.0    3  \n",
       "98    1.0  1.0  3.0    0  \n",
       "99    1.0  0.0  3.0    0  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleveland_df = pd.read_csv(\"processed.heading.cleveland.data\")\n",
    "cleveland_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values and replace any with the mean value of the column if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by replacing non-numeric values with NaN,\n",
    "    and filling NaNs with the mean of each column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Replace '?' with NaN\n",
    "    df.replace('?', pd.NA, inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric, forcing errors to NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill NaN values with the mean of each column\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the function\n",
    "cleveland_df = replace_data(cleveland_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: DataFrame, target_column: str, test_size: float = 0.3, random_state: int = 42) -> Tuple[DataFrame, DataFrame, Series, Series]:\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training and testing sets for features and target.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target_column (str): The name of the column to be used as the target variable.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[DataFrame, DataFrame, Series, Series]: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Call the function\n",
    "X_train, X_test, y_train, y_test = split_data(cleveland_df, 'num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply combined oversampling and undersampling for the datasets (SMOTEENN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smoteenn(X: DataFrame, y: Series) -> Tuple[DataFrame, Series]:\n",
    "    \"\"\"\n",
    "    Applies SMOTEENN to balance the classes in the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature data for training.\n",
    "    y (Series): The target data for training.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[DataFrame, Series]: Resampled X and y.\n",
    "    \"\"\"\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smoteenn.fit_resample(X, y)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Call the function\n",
    "X_train_resampled, y_train_resampled = apply_smoteenn(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. (5 pts) Create a decision tree model tuned to the best of your abilities. Explain how you tuned it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81        48\n",
      "           1       0.22      0.12      0.15        17\n",
      "           2       0.31      0.42      0.36        12\n",
      "           3       0.12      0.10      0.11        10\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.56        91\n",
      "   macro avg       0.28      0.31      0.29        91\n",
      "weighted avg       0.49      0.56      0.52        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def decision_tree(X_train: DataFrame, y_train: Series, X_test: DataFrame, y_test: Series) -> None:\n",
    "    \"\"\"\n",
    "    Trains a Decision Tree Classifier, makes predictions on the test set, and prints the classification report.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): Training feature set.\n",
    "    y_train (Series): Training target labels.\n",
    "    X_test (DataFrame): Test feature set.\n",
    "    y_test (Series): Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model = tree.DecisionTreeClassifier(\n",
    "        max_depth=5,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=3,\n",
    "        max_features=3,\n",
    "        criterion='entropy',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Call the function\n",
    "decision_tree(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for my decision tree was to maximize the weghted avg recall. The first thing that I did when tuning the decision tree model was to tweak the max_depth of the tree. This is because if the tree is too deep, then there is a risk over overfitment; therefore, it is crucial to find the max_depth that yields the best effect without overfitting or underfitting. Then I started tweaking the min_samples_split and the min_samples_leaf which should reduce the amount of overfitting; however, this tweaking yielded no benifit to the weighted avg recall score. So I moved on to tuning the max_features in order to limit the number of features considered at each split. This prevents the model from fitting noise. Lastly, I tweaked the criterion to find the highest recall score using 'gini' and 'entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. (5 pts) Create a random forest model tuned to the best of your abilities. Explain how you tuned it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80        48\n",
      "           1       0.42      0.29      0.34        17\n",
      "           2       0.40      0.17      0.24        12\n",
      "           3       0.20      0.20      0.20        10\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.59        91\n",
      "   macro avg       0.34      0.32      0.32        91\n",
      "weighted avg       0.52      0.59      0.54        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def random_forest(X_train: DataFrame, y_train: Series, X_test: DataFrame, y_test: Series) -> None:\n",
    "    \"\"\"\n",
    "    Trains a Random Forest Classifier, makes predictions on the test set, and prints the classification report.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): Training feature set.\n",
    "    y_train (Series): Training target labels.\n",
    "    X_test (DataFrame): Test feature set.\n",
    "    y_test (Series): Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    random = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    )\n",
    "    random.fit(X_train, y_train)\n",
    "    y_pred = random.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the goal for this random forest model will be to maximize weighted avg recall. I only tweaked two things for this model (e.g., n_estimators and max_depth). I used the max depth that yielded the best results from the decision tree model and started tweaking the n_estimators. For n_estimators, I found that 50 yielded that best results. I tried adding some other parameters like min_samples_split which all yielded a lower recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 (5 pts) Create an xgboost model tuned to the best of your abilities. Explain how you tuned it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.98      0.84        48\n",
      "           1       0.22      0.35      0.27        17\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.58        91\n",
      "   macro avg       0.19      0.27      0.22        91\n",
      "weighted avg       0.43      0.58      0.49        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "def xgboost(X_train: DataFrame, y_train: Series, X_test: DataFrame, y_test: Series) -> None:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost Classifier, makes predictions on the test set, and prints the classification report.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): Training feature set.\n",
    "    y_train (Series): Training target labels.\n",
    "    X_test (DataFrame): Test feature set.\n",
    "    y_test (Series): Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        max_depth=4,\n",
    "        min_child_weight=35,\n",
    "        subsample=1.0,\n",
    "        eta=0.3,\n",
    "        n_estimators=500\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Call the function\n",
    "xgboost(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I aimed for a high weighted avg recall score. I first started by tweaking the max_depth of the model and found that 4 works pretty well again. Then I started tweaking the min_child_weight to reduct overfitting if there is any. A value of 35 worked well. Then I started tweaking the subsample value and found out that none of the values I put in were benifiting the model because the model is not overfitted anymore due to the aforementioned parameters that I used. Lastly I tried to tweak the n_estimators; however, none of the values that were inputed yielded a higher result for the weighted avg recall score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
