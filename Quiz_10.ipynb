{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts) Take a supervised learning model you recently created and apply one dimensionality-reduction technique. How did it influence the performance? Why do you think that is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main dataset\n",
    "\n",
    "card = pd.read_csv(\"Credit_card.csv\")\n",
    "\n",
    "# Import the label dataset\n",
    "\n",
    "label = pd.read_csv(\"Credit_card_label.csv\")\n",
    "\n",
    "# Merge the two dataframes\n",
    "\n",
    "data = card.merge(label, on = \"Ind_ID\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn relevant categorical values into numerical ones\n",
    "\n",
    "def column_encoder(data: pd.DataFrame, columns: List[str]) -> pd.DataFrame: \n",
    "    \n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for column in columns:\n",
    "    \n",
    "        data[column] = labelencoder.fit_transform(data[column])\n",
    "    \n",
    "    return data\n",
    "\n",
    "encode_column = [\"Car_Owner\", \"GENDER\", \"Propert_Owner\", \"Marital_status\", \"EDUCATION\", \"Housing_type\", \"Type_Occupation\", \"Type_Income\"]\n",
    "\n",
    "data = column_encoder(data, encode_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn relevant categorical values into numerical ones with null values being replaced my the mean\n",
    "\n",
    "def column_encoder(data: pd.DataFrame, columns: List[str]) -> pd.DataFrame: \n",
    "    \n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for column in columns:\n",
    "    \n",
    "        data[column] = labelencoder.fit_transform(data[column])\n",
    "\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "encode_column = [\"Car_Owner\", \"GENDER\", \"Propert_Owner\", \"Marital_status\", \"EDUCATION\", \"Housing_type\", \"Type_Occupation\", \"Type_Income\"]\n",
    "\n",
    "data = column_encoder(data, encode_column)\n",
    "\n",
    "data.fillna(data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply both oversampling and undersampling\n",
    "\n",
    "def apply_smoteenn(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    \n",
    "    X_resampled, y_resampled = smoteenn.fit_resample(X, y)\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance (accuracy): 0.8451612903225807\n"
     ]
    }
   ],
   "source": [
    "def logistic_model(data: pd.DataFrame, predicts: List[str], predicted: List[str], test_size: float = 0.2, random_state: int = 32) -> Tuple[LogisticRegression, float, str]:\n",
    "    \n",
    "    X = data[predicts]\n",
    "    \n",
    "    y = data[predicted].values.ravel() \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    \n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    X_train, y_train = apply_smoteenn(X_train, y_train)\n",
    "    \n",
    "    regression = LogisticRegression(random_state=random_state)\n",
    "    \n",
    "    regression.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regression.predict(X_test)\n",
    "    \n",
    "    regression_score = regression.score(X_test, y_test)\n",
    "    \n",
    "    print(\"The model performance (accuracy):\", regression_score)\n",
    "\n",
    "    return regression, regression_score\n",
    "\n",
    "regression, regression_score = logistic_model(data, [\"Car_Owner\", \"Propert_Owner\", \"GENDER\", \"EDUCATION\"], [\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance (accuracy): 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "def logistic_model(data: pd.DataFrame, predicts: List[str], predicted: List[str], test_size: float = 0.2, random_state: int = 32) -> Tuple[Pipeline, float, str]:\n",
    "    \n",
    "    X = data[predicts]\n",
    "    \n",
    "    y = data[predicted].values.ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    \n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    X_train, y_train = apply_smoteenn(X_train, y_train)\n",
    "    \n",
    "    steps = [('lle', LocallyLinearEmbedding(n_components=4)), ('m', LogisticRegression(random_state=random_state))]\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    regression_score = model.score(X_test, y_test)\n",
    "\n",
    "    print(\"The model performance (accuracy):\", regression_score)\n",
    "    \n",
    "    return model, regression_score\n",
    "\n",
    "model, regression_score = logistic_model(data, [\"Car_Owner\", \"Propert_Owner\", \"GENDER\", \"EDUCATION\"], [\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Linear Embedding increased the performance of the model. I used Locally Linear Embedding to cut down the number of features, which cleaned up my data by removing unnecessary noise and helped my logistic regression model perform better. It kept the important relationships between data points, making it easier for the model to tell the classes apart. Plus, having fewer dimensions made the model run faster and work better on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
